#!/usr/bin/env python

from __future__ import division

from Pegasus.DAX3 import *
import sys
import math
import os
import re
import time
import datetime
import subprocess
import ConfigParser



def add_merge_job(chunk, level, job_number, final):
    """
    adds a merge job
    """
    j = Job(name="merge.sh")
    out_file = File("merged-%d-%d.txt" %(level, job_number))
    if final:
        out_file = File("final_results.txt")
    j.uses(out_file, link=Link.OUTPUT, transfer=final)
    j.addArguments(out_file)
    for f in chunk:
        j.uses(f, link=Link.INPUT)
        j.addArguments(f)
    # set a priority to the jobs run in the order the wf was submitted
    j.addProfile(Profile(Namespace.CONDOR, "priority", base_priority + 10))
    dax.addJob(j)
    return out_file


def merge_outputs(outputs, level):
    """
    creates a set of small jobs to merge the tarball
    """
    max_files = 25
    new_outputs = []

    output_chunks = [outputs[i:i + max_files] for i in xrange(0, len(outputs), max_files)]

    job_count = 0
    for chunk in output_chunks:
        job_count = job_count + 1
        f = add_merge_job(chunk, level, job_count, False)
        new_outputs.append(f)

    # end condition - only one chunk
    if len(new_outputs) <= max_files:
        return add_merge_job(new_outputs, level + 1, 1, True)

    return merge_outputs(new_outputs, level + 1)
        


base_dir = os.getcwd()

run_id = sys.argv[1]
run_dir = sys.argv[2]    
model = sys.argv[3]
job_start = int(sys.argv[4])
job_end = int(sys.argv[5])

# Give all jobs a priority based on the current time. We do this as we want
# workflows to finish in the order we submitted them.
end_date = datetime.datetime(year=2025, month=1, day=1)
now_date = datetime.datetime.now()
delta = end_date - now_date
hours = delta.days * 24 + delta.seconds / 3600
base_priority = int(round(hours))

# Create a abstract dag
dax = ADAG("macsswig_simsaj")

# email notificiations for when the state of the workflow changes
dax.invoke('all', base_dir + "/tools/email-notify")
dax.invoke('all', base_dir + "/tools/data-backup " + os.environ["RUN_ID"] + " " + os.environ["SUBMIT_ENV"]) 

# Add executables to the DAX-level replica catalog
for exe_name in os.listdir("./wrappers/"):
    exe = Executable(name=exe_name, arch="x86_64", installed=False)
    exe.addPFN(PFN("file://" + base_dir + "/wrappers/" + exe_name, "local"))
    dax.addExecutable(exe)

# common inputs
model_file = File("model.tar.gz")
model_file.addPFN(PFN("file://" + run_dir + "/model.tar.gz", "local"))
dax.addFile(model_file)

snp_file = File("ftDNA_hg18_auto_all_uniqSNPS_rmbadsites_pruned.bed")
snp_file.addPFN(PFN("file://" + base_dir + "/../ftDNA_hg18_auto_all_uniqSNPS_rmbadsites_pruned.bed", "local"))
#snp_file = File("test_genome.bed")
#snp_file.addPFN(PFN("file://" + base_dir + "/../test_genome.bed", "local"))
dax.addFile(snp_file)

genome_results_out = []
input_id = 0
for job_id in range(job_start, job_end + 1):
    
    #step one
    macsargs_job = Job(name="macsargs.sh") ##### CREATE THIS WRAPPER
    macsargs_job.uses(model_file, link=Link.INPUT)
    macsargs_job.addArguments(model, str(job_id), "full")
    #macsargs_job.addArguments(model, str(job_id), "100000")

    # output files
    macsargs_file = File("macsargs_" + str(job_id) + ".txt")
    macsargs_job.uses(macsargs_file, link=Link.OUTPUT, transfer=False)
    
    macsargs_job.addProfile(Profile(Namespace.CONDOR, "priority", base_priority))
    
    dax.addJob(macsargs_job)
    
    #step two
    current_germline_out= []
    current_results_out = []
    for chr in range(1, 22+1):
        sim_job = Job(name="run-sim.sh") ### NEED TO UPDATE WRAPPER
        sim_job.uses(model_file, link=Link.INPUT)
        sim_job.uses(macsargs_file, link=Link.INPUT)
        sim_job.uses(snp_file, link=Link.INPUT)
        sim_job.addArguments(model, str(chr), macsargs_file, snp_file)

        # output files
        germline_chr_file = File("macs_asc_" + str(job_id) + "_chr" + str(chr) + ".match") ## MOVE OUT OF DIR
        sim_job.uses(germline_chr_file, link=Link.OUTPUT, transfer=False)
        results_chr_file = File("results_" + str(job_id) + "_chr" + str(chr) + ".txt") ## MOVE OUT OF DIR
        sim_job.uses(results_chr_file, link=Link.OUTPUT, transfer=False)

        # set a priority to the jobs run in the order the wf was submitted
        sim_job.addProfile(Profile(Namespace.CONDOR, "priority", base_priority))

        dax.addJob(sim_job)

        # keep a list of all the outputs - in the next step we will group them and process
        # recursivly
        current_germline_out.append(germline_chr_file)
        current_results_out.append(results_chr_file)

    #step three
    genome_stats_job = Job(name="genome_stats.sh")  ##### CREATE THIS WRAPPER
    genome_stats_job.uses(model_file, link=Link.INPUT)
    for germline_file in current_germline_out:
        genome_stats_job.uses(germline_file, link=Link.INPUT)
    for results_file in current_results_out:
        genome_stats_job.uses(results_file, link=Link.INPUT)
    genome_stats_job.addArguments(model, str(job_id))

    # output files
    genome_results_file = File("results_" + str(job_id) + ".txt")
    genome_stats_job.uses(genome_results_file, link=Link.OUTPUT, transfer=True)

    # set a priority to the jobs run in the order the wf was submitted
    genome_stats_job.addProfile(Profile(Namespace.CONDOR, "priority", base_priority))

    dax.addJob(genome_stats_job)

    genome_results_out.append(genome_results_file)


# recurisvly process the outputs - this is so that we can handle the large amount of files
final_out_file = merge_outputs(genome_results_out, 0)


# Write the DAX to stdout
f = open("dax.xml", "w")
dax.writeXML(f)
f.close()

sys.exit(0)


